{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ozawa-nami/test/blob/main/%E3%80%90%E5%85%B1%E6%9C%89%E7%94%A8%E3%80%91%E7%AB%B6%E5%90%88%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import gspread\n",
        "\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "\n",
        "\n",
        "!pip install selenium\n",
        "!pip install webdriver_manager pyvirtualdisplay\n",
        "!apt-get update\n",
        "!apt-get install -y xvfb wget\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!apt-get install -y ./google-chrome-stable_current_amd64.deb\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "# 仮想ディスプレイの起動\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "\n",
        "# Chrome のオプションとサービスの設定\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument('--headless')\n",
        "\n",
        "s = Service(ChromeDriverManager().install())\n",
        "\n",
        "# Chrome ブラウザの起動\n",
        "driver = webdriver.Chrome(service=s, options=options)\n"
      ],
      "metadata": {
        "id": "-ULOloWJhlQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#自分でURL入れる\n",
        "\n",
        "# 必要なライブラリのインストール\n",
        "!pip install gspread beautifulsoup4 requests\n",
        "\n",
        "import gspread\n",
        "import google.auth\n",
        "from google.colab import auth\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "import time\n",
        "import re\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import chardet\n",
        "import json  # JSONデータの処理用\n",
        "\n",
        "# Googleドライブの認証\n",
        "auth.authenticate_user()\n",
        "creds, _ = google.auth.default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "#★ スプレッドシートのURLとシート名\n",
        "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1HyK034F_9oTnRlPpRsWMHNpXILbnuR52MWn2Ueh5Lpw/edit#gid=1960992532\"\n",
        "worksheet_name = \"{エンジニアネガポジ}競合分析\"\n",
        "\n",
        "# スプレッドシートの読み込み\n",
        "sh = gc.open_by_url(spreadsheet_url)\n",
        "worksheet = sh.worksheet(worksheet_name)\n",
        "\n",
        "# A列からURLを取得（例として5行目から20行目までのURLを取得）\n",
        "urls = [worksheet.cell(i, 1).value for i in range(5, 25)]\n",
        "\n",
        "\n",
        "\n",
        "# classify_link関数の定義\n",
        "def classify_link(href, base_url):\n",
        "    parsed_url = urlparse(href)\n",
        "    if parsed_url.netloc == base_url:\n",
        "        return \"内部リンク\"\n",
        "    elif parsed_url.netloc:\n",
        "        return \"外部リンク\"\n",
        "    elif parsed_url.fragment:\n",
        "        return \"ページ内リンク\"\n",
        "    else:\n",
        "        return \"不明なリンク\"\n",
        "\n",
        "# A1セルからキーワードを取得\n",
        "keywords_string = worksheet.cell(1, 1).value\n",
        "# キーワードをカンマで区切ってリストに変換\n",
        "keywords_list = [keyword.strip() for keyword in keywords_string.split(',')]\n",
        "\n",
        "\n",
        "# 各URLに対してスクレイピングと情報取得\n",
        "for i, url in enumerate(urls, start=5):\n",
        "    if not url:\n",
        "        for j in range(2, 12):  # B列からL列まで\n",
        "            worksheet.update_cell(i, j, \"\")  # 空文字を書き込む\n",
        "        continue\n",
        "\n",
        "    # リクエストの送信\n",
        "    try:\n",
        "        res = requests.get(url)\n",
        "        res.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request error for {url}: {e}\")\n",
        "        for j in range(2, 12):  # B列からL列まで\n",
        "            worksheet.update_cell(i, j, \"\")  # 空文字を書き込む\n",
        "        continue\n",
        "\n",
        "    # ページの解析\n",
        "    soup = BeautifulSoup(res.text, 'html.parser')  # この行をforループ内に移動\n",
        "\n",
        "    # 文字コードを自動判別\n",
        "    detected_encoding = chardet.detect(res.content)['encoding']\n",
        "    # ページの解析\n",
        "    soup = BeautifulSoup(res.content, 'html.parser', from_encoding=detected_encoding)\n",
        "\n",
        "    # サイトのテキストを取得\n",
        "    site_text = soup.get_text()\n",
        "\n",
        "    # H1タイトル\n",
        "    h1_tag = soup.find('h1')\n",
        "    h1_title = h1_tag.text.strip() if h1_tag else \"N/A\"\n",
        "\n",
        "    # H2とH3の見出しの取得\n",
        "    headers = soup.find_all(['h2', 'h3'])\n",
        "    headers_list = []\n",
        "    for h in headers:\n",
        "        if h.name == 'h2':\n",
        "            headers_list.append(f\"H2: {h.text.strip()}\")\n",
        "        elif h.name == 'h3':\n",
        "            headers_list.append(f\"H3: {h.text.strip()}\")\n",
        "    headers_text = \"\\n\".join(headers_list)\n",
        "\n",
        "    # サイトの文字数\n",
        "    site_text_length = len(re.sub(r'\\s+', ' ', site_text))\n",
        "\n",
        "    # ディスクリプション（metaタグから取得）\n",
        "    description = soup.find('meta', attrs={'name': 'description'})\n",
        "    description_content = description.get('content').strip() if description else \"N/A\"\n",
        "\n",
        "    # リンク数のカウント\n",
        "    links = soup.find_all('a')\n",
        "    link_count = len(links)\n",
        "\n",
        "\n",
        "\n",
        "  # classify_link関数の定義\n",
        "\n",
        "    def classify_link(href, base_url):\n",
        "        parsed_url = urlparse(href)\n",
        "        if parsed_url.netloc == base_url:\n",
        "            return \"内部リンク\"\n",
        "        elif parsed_url.netloc and base_url not in parsed_url.netloc:\n",
        "            return \"外部リンク\"\n",
        "        elif parsed_url.path.startswith(\"/\"):\n",
        "            return \"内部リンク\"\n",
        "        elif parsed_url.fragment:\n",
        "            return \"ページ内リンク\"\n",
        "        else:\n",
        "            return \"不明なリンク\"\n",
        "\n",
        "\n",
        "    # 発リンクおよび内部リンクのカウント\n",
        "    base_url = urlparse(url).netloc\n",
        "    internal_links = []\n",
        "    external_links = []\n",
        "    article_links = []\n",
        "\n",
        "    for link in links:\n",
        "        href = link.get('href')\n",
        "        if href:\n",
        "            link_type = classify_link(href, base_url)\n",
        "            if link_type == \"内部リンク\":\n",
        "                internal_links.append((href, link.text))\n",
        "            elif link_type == \"外部リンク\":\n",
        "                external_links.append((href, link.text))\n",
        "            elif link_type == \"ページ内リンク\":\n",
        "                article_links.append((href, link.text))\n",
        "\n",
        "    # 外部リンクの整形\n",
        "    external_links_count = len(external_links)\n",
        "    external_links_text = \"\\n\".join([f\"{text}: {href}\" for href, text in external_links])\n",
        "\n",
        "    # 内部リンクの整形\n",
        "    internal_links_count = len(internal_links)\n",
        "    internal_links_text = \"\\n\".join([f\"{text}: {href}\" for href, text in internal_links])\n",
        "\n",
        "    # 記事内リンクの整形\n",
        "    article_links_count = len(article_links)\n",
        "    article_links_text = \"\\n\".join([f\"{text}: {href}\" for href, text in article_links])\n",
        "\n",
        "\n",
        "    # 画像リンクとその数の取得\n",
        "    images = soup.find_all('img')\n",
        "    image_count = len(images)\n",
        "    image_links = [img.get('src') for img in images if img.get('src')]\n",
        "    image_links_formatted = \"\\n\".join(image_links)\n",
        "\n",
        "    # 画像リンクのテキストを切り詰める\n",
        "    truncated_image_links_formatted = image_links_formatted[:50000]\n",
        "\n",
        "\n",
        "    # JSONデータを抽出\n",
        "    soup = BeautifulSoup(res.content, 'html.parser')\n",
        "    json_ld_scripts = soup.find_all('script', {'type': 'application/ld+json'})\n",
        "\n",
        "    # JSONデータが複数ある場合の処理\n",
        "    date_published = \"N/A\"\n",
        "    date_modified = \"N/A\"\n",
        "\n",
        "    for script in json_ld_scripts:\n",
        "        try:\n",
        "            json_data = json.loads(script.string)\n",
        "            if isinstance(json_data, list):\n",
        "                # JSONデータがリストの場合、各要素をチェック\n",
        "                for item in json_data:\n",
        "                    if item.get('@type') in ['BlogPosting', 'Article', 'WebPage']:\n",
        "                        date_published = item.get('datePublished', \"N/A\")\n",
        "                        date_modified = item.get('dateModified', \"N/A\")\n",
        "                        break\n",
        "            elif json_data.get('@type') in ['BlogPosting', 'Article', 'WebPage']:\n",
        "                date_published = json_data.get('datePublished', \"N/A\")\n",
        "                date_modified = json_data.get('dateModified', \"N/A\")\n",
        "            elif json_data.get('@graph'):\n",
        "                # グラフ構造を持つJSONデータの場合\n",
        "                graph_data = json_data.get('@graph')\n",
        "                for item in graph_data:\n",
        "                    if item.get('@type') in ['BlogPosting', 'Article', 'WebPage']:\n",
        "                        date_published = item.get('datePublished', \"N/A\")\n",
        "                        date_modified = item.get('dateModified', \"N/A\")\n",
        "                        break\n",
        "        except json.JSONDecodeError:\n",
        "            # JSONデータのパースに失敗した場合\n",
        "            continue\n",
        "        if date_published != \"N/A\" and date_modified != \"N/A\":\n",
        "            # 有効なデータが見つかった場合\n",
        "            break\n",
        "\n",
        "    # date_publishedとdate_modifiedを出力\n",
        "    print(f\"URL: {url}\")\n",
        "    print(f\"Date Published: {date_published}\")\n",
        "    print(f\"Date Modified: {date_modified}\")\n",
        "\n",
        "\n",
        "    # 各キーワードの出現数をカウント\n",
        "    keyword_counts = {}\n",
        "    for keyword in keywords_list:\n",
        "        keyword_count = site_text.lower().count(keyword.lower())  # 大小文字を無視してカウント\n",
        "        keyword_counts[keyword] = keyword_count\n",
        "    keyword_counts_text = \"\\n\".join([f\"{k}: {v}\" for k, v in keyword_counts.items()])\n",
        "\n",
        "    # 出力\n",
        "    worksheet.update_cell(i, 2, h1_title)  # B列にタイトルを書き込む\n",
        "    worksheet.update_cell(i, 3, headers_text)  # C列にH2H3情報を書き込む\n",
        "    worksheet.update_cell(i, 4, site_text_length)  # D列にサイトの文字数を書き込む\n",
        "    worksheet.update_cell(i, 5, description_content)  # E列にサイトのディスクリプションを書き込む\n",
        "    worksheet.update_cell(i, 6, f\"{external_links_count}\\n{external_links_text}\")  # F列に外部リンク数とリンクを書き込む\n",
        "\n",
        "    truncated_internal_links_text = internal_links_text[:50000]\n",
        "    worksheet.update_cell(i, 7, f\"{internal_links_count}\\n{truncated_internal_links_text}\")   # G列に内部リンク数とリンクを書き込む\n",
        "\n",
        "    # worksheet.update_cell(i, 7, f\"{internal_links_count}\\n{internal_links_text}\")  # G列に内部リンク数とリンクを書き込む\n",
        "\n",
        "    worksheet.update_cell(i, 8, article_links_count)  # H列にページ内リンク数を書き込む\n",
        "\n",
        "        # I列に画像数とリンクを書き込む（画像リンクのテキストを切り詰めたバージョンを使用）\n",
        "    worksheet.update_cell(i, 9, f\"{image_count}\\n{truncated_image_links_formatted}\")\n",
        "\n",
        "    worksheet.update_cell(i, 10, date_published)  # J列にDate Publishedを書き込む\n",
        "    worksheet.update_cell(i, 11, date_modified)  # K列にDate Modifiedを書き込む\n",
        "    worksheet.update_cell(i, 12, keyword_counts_text)  # L列にキーワード出現数を書き込む\n",
        "\n",
        "\n",
        "    # APIの利用制限を考慮して待ち時間を設ける\n",
        "    time.sleep(2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DB9bzgxeq0Hv",
        "outputId": "44061cef-8939-48d2-af21-9e7a6d6e2a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gspread in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from gspread) (2.17.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth->gspread) (0.5.0)\n",
            "URL: https://www.workport.co.jp/\n",
            "Date Published: N/A\n",
            "Date Modified: N/A\n",
            "URL: https://www.a-tm.co.jp/top/job-change/workport/\n",
            "Date Published: 2023-03-02T00:00:00+09:00\n",
            "Date Modified: 2023-10-10T16:46:07+09:00\n",
            "URL: https://axxis.co.jp/magazine/55349\n",
            "Date Published: 2022-06-14T16:39:55+09:00\n",
            "Date Modified: 2023-10-16T21:04:52+09:00\n",
            "URL: https://www.maneo.jp/career/jc-workport-reputation/\n",
            "Date Published: 2023-10-03T13:41:36+0900\n",
            "Date Modified: 2023-10-03T13:41:37+0900\n",
            "URL: https://www.jesra.or.jp/search/1239/\n",
            "Date Published: N/A\n",
            "Date Modified: N/A\n",
            "URL: https://prtimes.jp/main/html/searchrlp/company_id/39106\n",
            "Date Published: N/A\n",
            "Date Modified: N/A\n",
            "URL: https://www.your-intern.com/tenshoku/content/2744/\n",
            "Date Published: N/A\n",
            "Date Modified: N/A\n",
            "URL: https://m.facebook.com/workport/\n",
            "Date Published: N/A\n",
            "Date Modified: N/A\n",
            "URL: https://m.youtube.com/channel/UCxENWeGPMWlyCkcvs9L7AtA\n",
            "Date Published: N/A\n",
            "Date Modified: N/A\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-71fbfef62647>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mscript\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson_ld_scripts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mjson_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;31m# JSONデータがリストの場合、各要素をチェック\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n\u001b[0m\u001b[1;32m    340\u001b[0m                             f'not {s.__class__.__name__}')\n\u001b[1;32m    341\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetect_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'surrogatepass'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not NoneType"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A1セルに入っているKWの上位⚪︎位を分析\n",
        "\n",
        "import gspread\n",
        "import google.auth\n",
        "from google.colab import auth\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "import time\n",
        "import re\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import chardet\n",
        "\n",
        "# Googleドライブの認証\n",
        "auth.authenticate_user()\n",
        "creds, _ = google.auth.default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "#★ スプレッドシートのURLとシート名\n",
        "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1HyK034F_9oTnRlPpRsWMHNpXILbnuR52MWn2Ueh5Lpw/edit#gid=558593770\"\n",
        "worksheet_name = \"上位記事取得用_01\"\n",
        "\n",
        "# スプレッドシートの読み込み\n",
        "sh = gc.open_by_url(spreadsheet_url)\n",
        "worksheet = sh.worksheet(worksheet_name)\n",
        "\n",
        "# キーワードの取得\n",
        "keyword = worksheet.acell('A1').value\n",
        "keywords_list = keyword.split(\",\")\n",
        "\n",
        "# Google検索のURLを構築\n",
        "search_url = f\"https://www.google.com/search?q={keyword}&num=10\"\n",
        "\n",
        "# リクエストの送信\n",
        "try:\n",
        "    res = requests.get(search_url)\n",
        "    res.raise_for_status()\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Request error: {e}\")\n",
        "    exit()\n",
        "\n",
        "# ページの解析\n",
        "soup = BeautifulSoup(res.text, 'html.parser')\n",
        "\n",
        "# URLの取得\n",
        "url_cells = soup.select('.kCrYT > a')\n",
        "urls = []\n",
        "for url_cell in url_cells:\n",
        "    href = url_cell.get('href')\n",
        "    parsed_url = urlparse(href)\n",
        "    query_params = parse_qs(parsed_url.query)\n",
        "    if 'q' in query_params:\n",
        "        url = query_params['q'][0]\n",
        "        urls.append(url)\n",
        "\n",
        "# classify_link関数の定義\n",
        "def classify_link(href, base_url):\n",
        "    parsed_url = urlparse(href)\n",
        "    if parsed_url.netloc == base_url:\n",
        "        return \"内部リンク\"\n",
        "    elif parsed_url.netloc:\n",
        "        return \"外部リンク\"\n",
        "    elif parsed_url.fragment:\n",
        "        return \"ページ内リンク\"\n",
        "    else:\n",
        "        return \"不明なリンク\"\n",
        "\n",
        "# 上位10記事のURLをスプレッドシートに書き込む\n",
        "for i, url in enumerate(urls, start=5):\n",
        "    worksheet.update_cell(i, 1, url)\n",
        "\n",
        "print(\"URLの取得が完了しました。\")\n",
        "\n",
        "# 各URLに対してスクレイピングと情報取得\n",
        "for i, url in enumerate(urls, start=5):\n",
        "    if not url:\n",
        "        for j in range(2, 11):  # B列からJ列まで\n",
        "            worksheet.update_cell(i, j, \"\")  # 空文字を書き込む\n",
        "        continue\n",
        "\n",
        "    # リクエストの送信\n",
        "    try:\n",
        "        res = requests.get(url)\n",
        "        res.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request error for {url}: {e}\")\n",
        "        for j in range(2, 11):  # B列からJ列まで\n",
        "            worksheet.update_cell(i, j, \"\")  # 空文字を書き込む\n",
        "        continue\n",
        "\n",
        "    # 文字コードを自動判別\n",
        "    detected_encoding = chardet.detect(res.content)['encoding']\n",
        "    # ページの解析\n",
        "    soup = BeautifulSoup(res.content, 'html.parser', from_encoding=detected_encoding)\n",
        "\n",
        "    # サイトのテキストを取得\n",
        "    site_text = soup.get_text()\n",
        "\n",
        "    # H1タイトル\n",
        "    h1_tag = soup.find('h1')\n",
        "    h1_title = h1_tag.text.strip() if h1_tag else \"N/A\"\n",
        "\n",
        "    # H2とH3の見出しの取得\n",
        "    headers = soup.find_all(['h2', 'h3'])\n",
        "    headers_list = []\n",
        "    for h in headers:\n",
        "        if h.name == 'h2':\n",
        "            headers_list.append(f\"H2: {h.text.strip()}\")\n",
        "        elif h.name == 'h3':\n",
        "            headers_list.append(f\"H3: {h.text.strip()}\")\n",
        "    headers_text = \"\\n\".join(headers_list)\n",
        "\n",
        "    # サイトの文字数\n",
        "    site_text_length = len(re.sub(r'\\s+', ' ', site_text))\n",
        "\n",
        "    # ディスクリプション（metaタグから取得）\n",
        "    description = soup.find('meta', attrs={'name': 'description'})\n",
        "    description_content = description.get('content').strip() if description else \"N/A\"\n",
        "\n",
        "    # リンク数のカウント\n",
        "    links = soup.find_all('a')\n",
        "    link_count = len(links)\n",
        "\n",
        "\n",
        "\n",
        "  # classify_link関数の定義\n",
        "    def classify_link(href, base_url):\n",
        "        parsed_url = urlparse(href)\n",
        "        if parsed_url.netloc == base_url:\n",
        "            return \"内部リンク\"\n",
        "        elif parsed_url.netloc and base_url not in parsed_url.netloc:\n",
        "            return \"外部リンク\"\n",
        "        elif parsed_url.path.startswith(\"/\"):\n",
        "            return \"内部リンク\"\n",
        "        elif parsed_url.fragment:\n",
        "            return \"ページ内リンク\"\n",
        "        else:\n",
        "            return \"不明なリンク\"\n",
        "\n",
        "\n",
        "    # 発リンクおよび内部リンクのカウント\n",
        "    base_url = urlparse(url).netloc\n",
        "    internal_links = []\n",
        "    external_links = []\n",
        "    article_links = []\n",
        "\n",
        "    for link in links:\n",
        "        href = link.get('href')\n",
        "        if href:\n",
        "            link_type = classify_link(href, base_url)\n",
        "            if link_type == \"内部リンク\":\n",
        "                internal_links.append((href, link.text))\n",
        "            elif link_type == \"外部リンク\":\n",
        "                external_links.append((href, link.text))\n",
        "            elif link_type == \"ページ内リンク\":\n",
        "                article_links.append((href, link.text))\n",
        "\n",
        "    # 外部リンクの整形\n",
        "    external_links_count = len(external_links)\n",
        "    external_links_text = \"\\n\".join([f\"{text}: {href}\" for href, text in external_links])\n",
        "\n",
        "    # 内部リンクの整形\n",
        "    internal_links_count = len(internal_links)\n",
        "    internal_links_text = \"\\n\".join([f\"{text}: {href}\" for href, text in internal_links])\n",
        "\n",
        "    # 記事内リンクの整形\n",
        "    article_links_count = len(article_links)\n",
        "    article_links_text = \"\\n\".join([f\"{text}: {href}\" for href, text in article_links])\n",
        "\n",
        "    # 画像リンクとその数の取得\n",
        "    images = soup.find_all('img')\n",
        "    image_count = len(images)\n",
        "    image_links = [img.get('src') for img in images if img.get('src')]\n",
        "    image_links_formatted = \"\\n\".join(image_links)\n",
        "\n",
        "    # 各キーワードの出現数をカウント\n",
        "    keyword_counts = {}\n",
        "    for keyword in keywords_list:\n",
        "        keyword_count = site_text.count(keyword)\n",
        "        keyword_counts[keyword] = keyword_count\n",
        "    keyword_counts_text = \"\\n\".join([f\"{k}: {v}\" for k, v in keyword_counts.items()])\n",
        "\n",
        "    # 出力\n",
        "    worksheet.update_cell(i, 2, h1_title)  # B列にタイトルを書き込む\n",
        "    worksheet.update_cell(i, 3, headers_text)  # C列にH2H3情報を書き込む\n",
        "    worksheet.update_cell(i, 4, site_text_length)  # D列にサイトの文字数を書き込む\n",
        "    worksheet.update_cell(i, 5, description_content)  # E列にサイトのディスクリプションを書き込む\n",
        "    worksheet.update_cell(i, 6, f\"{external_links_count}\\n{external_links_text}\")  # F列に外部リンク数とリンクを書き込む\n",
        "\n",
        "    truncated_internal_links_text = internal_links_text[:50000]\n",
        "    worksheet.update_cell(i, 7, f\"{internal_links_count}\\n{truncated_internal_links_text}\")\n",
        "\n",
        "    # worksheet.update_cell(i, 7, f\"{internal_links_count}\\n{internal_links_text}\")  # G列に内部リンク数とリンクを書き込む\n",
        "\n",
        "    worksheet.update_cell(i, 8, article_links_count)  # H列にページ内リンク数を書き込む\n",
        "    worksheet.update_cell(i, 9, f\"{image_count}\\n{image_links_formatted}\")  # I列に画像数とリンクを書き込む\n",
        "    worksheet.update_cell(i, 10, keyword_counts_text)  # J列にキーワード出現数を書き込む\n",
        "\n",
        "    # APIの利用制限を考慮して待ち時間を設ける\n",
        "    time.sleep(2)\n"
      ],
      "metadata": {
        "id": "cM5HUOMnhmfl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b774dbc7-d5ad-4c3a-ab6e-f9cfb964d604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URLの取得が完了しました。\n",
            "Request error for https://exidea.co.jp/blog/job/job-change/it-tenshoku-agent/: HTTPSConnectionPool(host='exidea.co.jp', port=443): Max retries exceeded with url: /blog/job/job-change/it-tenshoku-agent/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\n",
            "Request error for https://mynavi-agent.jp/it/: 403 Client Error: Forbidden for url: https://mynavi-agent.jp/it/\n"
          ]
        }
      ]
    }
  ]
}