{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ozawa-nami/test/blob/main/%E3%80%90%E5%85%B1%E6%9C%89%E7%94%A8%E3%80%91%E7%AB%B6%E5%90%88%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import gspread\n",
        "\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "\n",
        "\n",
        "!pip install selenium\n",
        "!pip install webdriver_manager pyvirtualdisplay\n",
        "!apt-get update\n",
        "!apt-get install -y xvfb wget\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!apt-get install -y ./google-chrome-stable_current_amd64.deb\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "# 仮想ディスプレイの起動\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "\n",
        "# Chrome のオプションとサービスの設定\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument('--headless')\n",
        "\n",
        "s = Service(ChromeDriverManager().install())\n",
        "\n",
        "# Chrome ブラウザの起動\n",
        "driver = webdriver.Chrome(service=s, options=options)\n"
      ],
      "metadata": {
        "id": "-ULOloWJhlQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40fd5b53-ba80-4284-93ac-ec024cc06fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.14.0-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.22.2-py3-none-any.whl (400 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.2/400.2 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.4)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.1.3)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.14.0 trio-0.22.2 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "Collecting webdriver_manager\n",
            "  Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (2.31.0)\n",
            "Collecting python-dotenv (from webdriver_manager)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2023.7.22)\n",
            "Installing collected packages: pyvirtualdisplay, python-dotenv, webdriver_manager\n",
            "Successfully installed python-dotenv-1.0.0 pyvirtualdisplay-3.0 webdriver_manager-4.0.1\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,012 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [557 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,384 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,184 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,278 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,453 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,410 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [32.6 kB]\n",
            "Hit:17 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:18 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:19 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,231 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main amd64 Packages [1,145 kB]\n",
            "Fetched 12.0 MB in 4s (2,950 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "wget is already the newest version (1.21.2-2ubuntu1).\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 22 not upgraded.\n",
            "Need to get 7,814 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.2 [28.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.2 [864 kB]\n",
            "Fetched 7,814 kB in 1s (5,479 kB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 120874 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.2_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.2_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "--2023-10-31 02:05:54--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 173.194.215.190, 173.194.215.136, 173.194.215.91, ...\n",
            "Connecting to dl.google.com (dl.google.com)|173.194.215.190|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 104057764 (99M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb’\n",
            "\n",
            "google-chrome-stabl 100%[===================>]  99.24M   116MB/s    in 0.9s    \n",
            "\n",
            "2023-10-31 02:05:55 (116 MB/s) - ‘google-chrome-stable_current_amd64.deb’ saved [104057764/104057764]\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'google-chrome-stable' instead of './google-chrome-stable_current_amd64.deb'\n",
            "The following additional packages will be installed:\n",
            "  libu2f-udev libudev1 libvulkan1 mesa-vulkan-drivers systemd-hwe-hwdb udev\n",
            "The following NEW packages will be installed:\n",
            "  google-chrome-stable libu2f-udev libvulkan1 mesa-vulkan-drivers systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 6 newly installed, 0 to remove and 21 not upgraded.\n",
            "Need to get 11.3 MB/115 MB of archives.\n",
            "After this operation, 391 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.11 [78.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.11 [1,557 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libu2f-udev all 1.1.10-3build2 [4,190 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvulkan1 amd64 1.3.204.1-2 [128 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 mesa-vulkan-drivers amd64 23.0.4-0ubuntu1~22.04.1 [9,521 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.3 [2,908 B]\n",
            "Get:7 /content/google-chrome-stable_current_amd64.deb google-chrome-stable amd64 118.0.5993.117-1 [104 MB]\n",
            "Fetched 11.3 MB in 1s (10.2 MB/s)\n",
            "(Reading database ... 121437 files and directories currently installed.)\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.11_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.11) over (249.11-0ubuntu3.9) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.11) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 121437 files and directories currently installed.)\n",
            "Preparing to unpack .../0-udev_249.11-0ubuntu3.11_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.11) ...\n",
            "Selecting previously unselected package libu2f-udev.\n",
            "Preparing to unpack .../1-libu2f-udev_1.1.10-3build2_all.deb ...\n",
            "Unpacking libu2f-udev (1.1.10-3build2) ...\n",
            "Selecting previously unselected package libvulkan1:amd64.\n",
            "Preparing to unpack .../2-libvulkan1_1.3.204.1-2_amd64.deb ...\n",
            "Unpacking libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Selecting previously unselected package google-chrome-stable.\n",
            "Preparing to unpack .../3-google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (118.0.5993.117-1) ...\n",
            "Selecting previously unselected package mesa-vulkan-drivers:amd64.\n",
            "Preparing to unpack .../4-mesa-vulkan-drivers_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking mesa-vulkan-drivers:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../5-systemd-hwe-hwdb_249.11.3_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.3) ...\n",
            "Setting up udev (249.11-0ubuntu3.11) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.3) ...\n",
            "Setting up mesa-vulkan-drivers:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Setting up libu2f-udev (1.1.10-3build2) ...\n",
            "Setting up google-chrome-stable (118.0.5993.117-1) ...\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/google-chrome (google-chrome) in auto mode\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#自分でURL入れる\n",
        "\n",
        "# 必要なライブラリのインストール\n",
        "!pip install gspread beautifulsoup4 requests\n",
        "\n",
        "import gspread\n",
        "import google.auth\n",
        "from google.colab import auth\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "import time\n",
        "import re\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import chardet\n",
        "import json  # JSONデータの処理用\n",
        "\n",
        "# Googleドライブの認証\n",
        "auth.authenticate_user()\n",
        "creds, _ = google.auth.default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "#★ スプレッドシートのURLとシート名\n",
        "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1OgdSqsKCGpteswYS2slz0fvwNi5kQzw8QDA1OWEWQQQ/edit#gid=1221780822\"\n",
        "worksheet_name = \"競合分析\"\n",
        "\n",
        "# スプレッドシートの読み込み\n",
        "sh = gc.open_by_url(spreadsheet_url)\n",
        "worksheet = sh.worksheet(worksheet_name)\n",
        "\n",
        "# A列からURLを取得（例として5行目から20行目までのURLを取得）\n",
        "urls = [worksheet.cell(i, 1).value for i in range(5, 25)]\n",
        "\n",
        "\n",
        "\n",
        "# classify_link関数の定義\n",
        "def classify_link(href, base_url):\n",
        "    parsed_url = urlparse(href)\n",
        "    if parsed_url.netloc == base_url:\n",
        "        return \"内部リンク\"\n",
        "    elif parsed_url.netloc:\n",
        "        return \"外部リンク\"\n",
        "    elif parsed_url.fragment:\n",
        "        return \"ページ内リンク\"\n",
        "    else:\n",
        "        return \"不明なリンク\"\n",
        "\n",
        "# A1セルからキーワードを取得\n",
        "keywords_string = worksheet.cell(1, 1).value\n",
        "# キーワードをカンマで区切ってリストに変換\n",
        "keywords_list = [keyword.strip() for keyword in keywords_string.split(',')]\n",
        "\n",
        "\n",
        "# 各URLに対してスクレイピングと情報取得\n",
        "for i, url in enumerate(urls, start=5):\n",
        "    if not url:\n",
        "        for j in range(2, 12):  # B列からL列まで\n",
        "            worksheet.update_cell(i, j, \"\")  # 空文字を書き込む\n",
        "        continue\n",
        "\n",
        "    # リクエストの送信\n",
        "    try:\n",
        "        res = requests.get(url)\n",
        "        res.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request error for {url}: {e}\")\n",
        "        for j in range(2, 12):  # B列からL列まで\n",
        "            worksheet.update_cell(i, j, \"\")  # 空文字を書き込む\n",
        "        continue\n",
        "\n",
        "    # ページの解析\n",
        "    soup = BeautifulSoup(res.text, 'html.parser')  # この行をforループ内に移動\n",
        "\n",
        "    # 文字コードを自動判別\n",
        "    detected_encoding = chardet.detect(res.content)['encoding']\n",
        "    # ページの解析\n",
        "    soup = BeautifulSoup(res.content, 'html.parser', from_encoding=detected_encoding)\n",
        "\n",
        "    # サイトのテキストを取得\n",
        "    site_text = soup.get_text()\n",
        "\n",
        "    # H1タイトル\n",
        "    h1_tag = soup.find('h1')\n",
        "    h1_title = h1_tag.text.strip() if h1_tag else \"N/A\"\n",
        "\n",
        "    # H2とH3の見出しの取得\n",
        "    headers = soup.find_all(['h2', 'h3'])\n",
        "    headers_list = []\n",
        "    for h in headers:\n",
        "        if h.name == 'h2':\n",
        "            headers_list.append(f\"H2: {h.text.strip()}\")\n",
        "        elif h.name == 'h3':\n",
        "            headers_list.append(f\"H3: {h.text.strip()}\")\n",
        "    headers_text = \"\\n\".join(headers_list)\n",
        "\n",
        "    # サイトの文字数\n",
        "    site_text_length = len(re.sub(r'\\s+', ' ', site_text))\n",
        "\n",
        "    # ディスクリプション（metaタグから取得）\n",
        "    description = soup.find('meta', attrs={'name': 'description'})\n",
        "    description_content = description.get('content').strip() if description else \"N/A\"\n",
        "\n",
        "    # リンク数のカウント\n",
        "    links = soup.find_all('a')\n",
        "    link_count = len(links)\n",
        "\n",
        "\n",
        "\n",
        "  # classify_link関数の定義\n",
        "\n",
        "    def classify_link(href, base_url):\n",
        "        parsed_url = urlparse(href)\n",
        "        if parsed_url.netloc == base_url:\n",
        "            return \"内部リンク\"\n",
        "        elif parsed_url.netloc and base_url not in parsed_url.netloc:\n",
        "            return \"外部リンク\"\n",
        "        elif parsed_url.path.startswith(\"/\"):\n",
        "            return \"内部リンク\"\n",
        "        elif parsed_url.fragment:\n",
        "            return \"ページ内リンク\"\n",
        "        else:\n",
        "            return \"不明なリンク\"\n",
        "\n",
        "\n",
        "    # 発リンクおよび内部リンクのカウント\n",
        "    base_url = urlparse(url).netloc\n",
        "    internal_links = []\n",
        "    external_links = []\n",
        "    article_links = []\n",
        "\n",
        "    for link in links:\n",
        "        href = link.get('href')\n",
        "        if href:\n",
        "            link_type = classify_link(href, base_url)\n",
        "            if link_type == \"内部リンク\":\n",
        "                internal_links.append((href, link.text))\n",
        "            elif link_type == \"外部リンク\":\n",
        "                external_links.append((href, link.text))\n",
        "            elif link_type == \"ページ内リンク\":\n",
        "                article_links.append((href, link.text))\n",
        "\n",
        "    # 外部リンクの整形\n",
        "    external_links_count = len(external_links)\n",
        "    external_links_text = \"\\n\".join([f\"{text}: {href}\" for href, text in external_links])\n",
        "\n",
        "    # 内部リンクの整形\n",
        "    internal_links_count = len(internal_links)\n",
        "    internal_links_text = \"\\n\".join([f\"{text}: {href}\" for href, text in internal_links])\n",
        "\n",
        "    # 記事内リンクの整形\n",
        "    article_links_count = len(article_links)\n",
        "    article_links_text = \"\\n\".join([f\"{text}: {href}\" for href, text in article_links])\n",
        "\n",
        "\n",
        "    # 画像リンクとその数の取得\n",
        "    images = soup.find_all('img')\n",
        "    image_count = len(images)\n",
        "    image_links = [img.get('src') for img in images if img.get('src')]\n",
        "    image_links_formatted = \"\\n\".join(image_links)\n",
        "\n",
        "    # 画像リンクのテキストを切り詰める\n",
        "    truncated_image_links_formatted = image_links_formatted[:50000]\n",
        "\n",
        "\n",
        "    # JSONデータを抽出\n",
        "    soup = BeautifulSoup(res.content, 'html.parser')\n",
        "    json_ld_scripts = soup.find_all('script', {'type': 'application/ld+json'})\n",
        "\n",
        "    # JSONデータが複数ある場合の処理\n",
        "    date_published = \"N/A\"\n",
        "    date_modified = \"N/A\"\n",
        "\n",
        "    for script in json_ld_scripts:\n",
        "        try:\n",
        "            json_data = json.loads(script.string)\n",
        "            if isinstance(json_data, list):\n",
        "                # JSONデータがリストの場合、各要素をチェック\n",
        "                for item in json_data:\n",
        "                    if item.get('@type') in ['BlogPosting', 'Article', 'WebPage']:\n",
        "                        date_published = item.get('datePublished', \"N/A\")\n",
        "                        date_modified = item.get('dateModified', \"N/A\")\n",
        "                        break\n",
        "            elif json_data.get('@type') in ['BlogPosting', 'Article', 'WebPage']:\n",
        "                date_published = json_data.get('datePublished', \"N/A\")\n",
        "                date_modified = json_data.get('dateModified', \"N/A\")\n",
        "            elif json_data.get('@graph'):\n",
        "                # グラフ構造を持つJSONデータの場合\n",
        "                graph_data = json_data.get('@graph')\n",
        "                for item in graph_data:\n",
        "                    if item.get('@type') in ['BlogPosting', 'Article', 'WebPage']:\n",
        "                        date_published = item.get('datePublished', \"N/A\")\n",
        "                        date_modified = item.get('dateModified', \"N/A\")\n",
        "                        break\n",
        "        except json.JSONDecodeError:\n",
        "            # JSONデータのパースに失敗した場合\n",
        "            continue\n",
        "        if date_published != \"N/A\" and date_modified != \"N/A\":\n",
        "            # 有効なデータが見つかった場合\n",
        "            break\n",
        "\n",
        "    # date_publishedとdate_modifiedを出力\n",
        "    print(f\"URL: {url}\")\n",
        "    print(f\"Date Published: {date_published}\")\n",
        "    print(f\"Date Modified: {date_modified}\")\n",
        "\n",
        "\n",
        "    # 各キーワードの出現数をカウント\n",
        "    keyword_counts = {}\n",
        "    for keyword in keywords_list:\n",
        "        keyword_count = site_text.lower().count(keyword.lower())  # 大小文字を無視してカウント\n",
        "        keyword_counts[keyword] = keyword_count\n",
        "    keyword_counts_text = \"\\n\".join([f\"{k}: {v}\" for k, v in keyword_counts.items()])\n",
        "\n",
        "    # 出力\n",
        "    worksheet.update_cell(i, 2, h1_title)  # B列にタイトルを書き込む\n",
        "    worksheet.update_cell(i, 3, headers_text)  # C列にH2H3情報を書き込む\n",
        "    worksheet.update_cell(i, 4, site_text_length)  # D列にサイトの文字数を書き込む\n",
        "    worksheet.update_cell(i, 5, description_content)  # E列にサイトのディスクリプションを書き込む\n",
        "    worksheet.update_cell(i, 6, f\"{external_links_count}\\n{external_links_text}\")  # F列に外部リンク数とリンクを書き込む\n",
        "\n",
        "    truncated_internal_links_text = internal_links_text[:50000]\n",
        "    worksheet.update_cell(i, 7, f\"{internal_links_count}\\n{truncated_internal_links_text}\")   # G列に内部リンク数とリンクを書き込む\n",
        "\n",
        "    # worksheet.update_cell(i, 7, f\"{internal_links_count}\\n{internal_links_text}\")  # G列に内部リンク数とリンクを書き込む\n",
        "\n",
        "    worksheet.update_cell(i, 8, article_links_count)  # H列にページ内リンク数を書き込む\n",
        "\n",
        "        # I列に画像数とリンクを書き込む（画像リンクのテキストを切り詰めたバージョンを使用）\n",
        "    worksheet.update_cell(i, 9, f\"{image_count}\\n{truncated_image_links_formatted}\")\n",
        "\n",
        "    worksheet.update_cell(i, 10, date_published)  # J列にDate Publishedを書き込む\n",
        "    worksheet.update_cell(i, 11, date_modified)  # K列にDate Modifiedを書き込む\n",
        "    worksheet.update_cell(i, 12, keyword_counts_text)  # L列にキーワード出現数を書き込む\n",
        "\n",
        "\n",
        "    # APIの利用制限を考慮して待ち時間を設ける\n",
        "    time.sleep(2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DB9bzgxeq0Hv",
        "outputId": "640e7d0b-7834-4539-a896-c64753a3865c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gspread in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from gspread) (2.17.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth->gspread) (0.5.0)\n",
            "URL: https://resemom.jp/manabi/engineer-agent/\n",
            "Date Published: N/A\n",
            "Date Modified: N/A\n",
            "Request error for https://exidea.co.jp/blog/job/job-change/it-tenshoku-agent/: HTTPSConnectionPool(host='exidea.co.jp', port=443): Max retries exceeded with url: /blog/job/job-change/it-tenshoku-agent/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\n",
            "URL: https://asiro.co.jp/media-career/188/\n",
            "Date Published: 2022-07-07T12:43:52+09:00\n",
            "Date Modified: 2023-10-29T14:30:43+09:00\n",
            "Request error for https://mynavi-agent.jp/it/: 403 Client Error: Forbidden for url: https://mynavi-agent.jp/it/\n",
            "URL: https://doda.jp/engineer/consultant/\n",
            "Date Published: N/A\n",
            "Date Modified: N/A\n",
            "URL: https://www.a-tm.co.jp/top/job-change/career-change-it/\n",
            "Date Published: 2023-05-07T00:00:00+09:00\n",
            "Date Modified: 2023-10-30T17:20:49+09:00\n",
            "URL: https://axxis.co.jp/magazine/23374\n",
            "Date Published: 2019-06-06T10:10:40+09:00\n",
            "Date Modified: 2023-10-16T01:01:36+09:00\n",
            "URL: https://unison-career.com/engineer-media/7458/\n",
            "Date Published: 2023-10-28T01:00:38+00:00\n",
            "Date Modified: 2023-10-28T02:30:38+00:00\n",
            "URL: https://www.rise-square.jp/contents/tensyoku/webagent_osusume.php\n",
            "Date Published: N/A\n",
            "Date Modified: N/A\n",
            "URL: https://engineer-shukatu.jp/media/it-tenshoku-agent/\n",
            "Date Published: N/A\n",
            "Date Modified: N/A\n",
            "URL: https://www.geekly.co.jp/\n",
            "Date Published: N/A\n",
            "Date Modified: N/A\n",
            "URL: https://www.r-agent.com/it_engineer/\n",
            "Date Published: N/A\n",
            "Date Modified: N/A\n",
            "Request error for https://coeteco.jp/articles/11135: 403 Client Error: Forbidden for url: https://coeteco.jp/articles/11135\n",
            "Request error for https://my-best.com/13251: 403 Client Error: Forbidden for url: https://my-best.com/13251\n",
            "URL: https://career-theory.net/it-job-change-agent-2071\n",
            "Date Published: 2023-09-14T20:00:00+09:00\n",
            "Date Modified: 2023-10-12T10:58:20+09:00\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "APIError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f41cc9882f9b>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m# I列に画像数とリンクを書き込む（画像リンクのテキストを切り詰めたバージョンを使用）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mworksheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{image_count}\\n{truncated_image_links_formatted}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mworksheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_published\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# J列にDate Publishedを書き込む\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gspread/models.py\u001b[0m in \u001b[0;36mupdate_cell\u001b[0;34m(self, row, col, value)\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0mrange_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabsolute_range_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrowcol_to_a1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m         data = self.spreadsheet.values_update(\n\u001b[0m\u001b[1;32m    887\u001b[0m             \u001b[0mrange_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m             params={\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gspread/models.py\u001b[0m in \u001b[0;36mvalues_update\u001b[0;34m(self, range, params, body)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \"\"\"\n\u001b[1;32m    240\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSPREADSHEET_VALUES_URL\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'put'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gspread/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, endpoint, params, data, json, files, headers)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlist_spreadsheet_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAPIError\u001b[0m: {'code': 400, 'message': '入力内容が 1 つのセルに最大 50000 文字の制限を超えています。', 'status': 'INVALID_ARGUMENT'}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A1セルに入っているKWの上位⚪︎位を分析\n",
        "\n",
        "import gspread\n",
        "import google.auth\n",
        "from google.colab import auth\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "import time\n",
        "import re\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import chardet\n",
        "\n",
        "# Googleドライブの認証\n",
        "auth.authenticate_user()\n",
        "creds, _ = google.auth.default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "#★ スプレッドシートのURLとシート名\n",
        "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1OgdSqsKCGpteswYS2slz0fvwNi5kQzw8QDA1OWEWQQQ/edit#gid=1221780822\"\n",
        "worksheet_name = \"競合分析\"\n",
        "\n",
        "# スプレッドシートの読み込み\n",
        "sh = gc.open_by_url(spreadsheet_url)\n",
        "worksheet = sh.worksheet(worksheet_name)\n",
        "\n",
        "# キーワードの取得\n",
        "keyword = worksheet.acell('A1').value\n",
        "keywords_list = keyword.split(\",\")\n",
        "\n",
        "# Google検索のURLを構築\n",
        "search_url = f\"https://www.google.com/search?q={keyword}&num=10\"\n",
        "\n",
        "# リクエストの送信\n",
        "try:\n",
        "    res = requests.get(search_url)\n",
        "    res.raise_for_status()\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Request error: {e}\")\n",
        "    exit()\n",
        "\n",
        "# ページの解析\n",
        "soup = BeautifulSoup(res.text, 'html.parser')\n",
        "\n",
        "# URLの取得\n",
        "url_cells = soup.select('.kCrYT > a')\n",
        "urls = []\n",
        "for url_cell in url_cells:\n",
        "    href = url_cell.get('href')\n",
        "    parsed_url = urlparse(href)\n",
        "    query_params = parse_qs(parsed_url.query)\n",
        "    if 'q' in query_params:\n",
        "        url = query_params['q'][0]\n",
        "        urls.append(url)\n",
        "\n",
        "# classify_link関数の定義\n",
        "def classify_link(href, base_url):\n",
        "    parsed_url = urlparse(href)\n",
        "    if parsed_url.netloc == base_url:\n",
        "        return \"内部リンク\"\n",
        "    elif parsed_url.netloc:\n",
        "        return \"外部リンク\"\n",
        "    elif parsed_url.fragment:\n",
        "        return \"ページ内リンク\"\n",
        "    else:\n",
        "        return \"不明なリンク\"\n",
        "\n",
        "# 上位10記事のURLをスプレッドシートに書き込む\n",
        "for i, url in enumerate(urls, start=5):\n",
        "    worksheet.update_cell(i, 1, url)\n",
        "\n",
        "print(\"URLの取得が完了しました。\")\n",
        "\n",
        "# 各URLに対してスクレイピングと情報取得\n",
        "for i, url in enumerate(urls, start=5):\n",
        "    if not url:\n",
        "        for j in range(2, 11):  # B列からJ列まで\n",
        "            worksheet.update_cell(i, j, \"\")  # 空文字を書き込む\n",
        "        continue\n",
        "\n",
        "    # リクエストの送信\n",
        "    try:\n",
        "        res = requests.get(url)\n",
        "        res.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request error for {url}: {e}\")\n",
        "        for j in range(2, 11):  # B列からJ列まで\n",
        "            worksheet.update_cell(i, j, \"\")  # 空文字を書き込む\n",
        "        continue\n",
        "\n",
        "    # 文字コードを自動判別\n",
        "    detected_encoding = chardet.detect(res.content)['encoding']\n",
        "    # ページの解析\n",
        "    soup = BeautifulSoup(res.content, 'html.parser', from_encoding=detected_encoding)\n",
        "\n",
        "    # サイトのテキストを取得\n",
        "    site_text = soup.get_text()\n",
        "\n",
        "    # H1タイトル\n",
        "    h1_tag = soup.find('h1')\n",
        "    h1_title = h1_tag.text.strip() if h1_tag else \"N/A\"\n",
        "\n",
        "    # H2とH3の見出しの取得\n",
        "    headers = soup.find_all(['h2', 'h3'])\n",
        "    headers_list = []\n",
        "    for h in headers:\n",
        "        if h.name == 'h2':\n",
        "            headers_list.append(f\"H2: {h.text.strip()}\")\n",
        "        elif h.name == 'h3':\n",
        "            headers_list.append(f\"H3: {h.text.strip()}\")\n",
        "    headers_text = \"\\n\".join(headers_list)\n",
        "\n",
        "    # サイトの文字数\n",
        "    site_text_length = len(re.sub(r'\\s+', ' ', site_text))\n",
        "\n",
        "    # ディスクリプション（metaタグから取得）\n",
        "    description = soup.find('meta', attrs={'name': 'description'})\n",
        "    description_content = description.get('content').strip() if description else \"N/A\"\n",
        "\n",
        "    # リンク数のカウント\n",
        "    links = soup.find_all('a')\n",
        "    link_count = len(links)\n",
        "\n",
        "\n",
        "\n",
        "  # classify_link関数の定義\n",
        "    def classify_link(href, base_url):\n",
        "        parsed_url = urlparse(href)\n",
        "        if parsed_url.netloc == base_url:\n",
        "            return \"内部リンク\"\n",
        "        elif parsed_url.netloc and base_url not in parsed_url.netloc:\n",
        "            return \"外部リンク\"\n",
        "        elif parsed_url.path.startswith(\"/\"):\n",
        "            return \"内部リンク\"\n",
        "        elif parsed_url.fragment:\n",
        "            return \"ページ内リンク\"\n",
        "        else:\n",
        "            return \"不明なリンク\"\n",
        "\n",
        "\n",
        "    # 発リンクおよび内部リンクのカウント\n",
        "    base_url = urlparse(url).netloc\n",
        "    internal_links = []\n",
        "    external_links = []\n",
        "    article_links = []\n",
        "\n",
        "    for link in links:\n",
        "        href = link.get('href')\n",
        "        if href:\n",
        "            link_type = classify_link(href, base_url)\n",
        "            if link_type == \"内部リンク\":\n",
        "                internal_links.append((href, link.text))\n",
        "            elif link_type == \"外部リンク\":\n",
        "                external_links.append((href, link.text))\n",
        "            elif link_type == \"ページ内リンク\":\n",
        "                article_links.append((href, link.text))\n",
        "\n",
        "    # 外部リンクの整形\n",
        "    external_links_count = len(external_links)\n",
        "    external_links_text = \"\\n\".join([f\"{text}: {href}\" for href, text in external_links])\n",
        "\n",
        "    # 内部リンクの整形\n",
        "    internal_links_count = len(internal_links)\n",
        "    internal_links_text = \"\\n\".join([f\"{text}: {href}\" for href, text in internal_links])\n",
        "\n",
        "    # 記事内リンクの整形\n",
        "    article_links_count = len(article_links)\n",
        "    article_links_text = \"\\n\".join([f\"{text}: {href}\" for href, text in article_links])\n",
        "\n",
        "    # 画像リンクとその数の取得\n",
        "    images = soup.find_all('img')\n",
        "    image_count = len(images)\n",
        "    image_links = [img.get('src') for img in images if img.get('src')]\n",
        "    image_links_formatted = \"\\n\".join(image_links)\n",
        "\n",
        "    # 各キーワードの出現数をカウント\n",
        "    keyword_counts = {}\n",
        "    for keyword in keywords_list:\n",
        "        keyword_count = site_text.count(keyword)\n",
        "        keyword_counts[keyword] = keyword_count\n",
        "    keyword_counts_text = \"\\n\".join([f\"{k}: {v}\" for k, v in keyword_counts.items()])\n",
        "\n",
        "    # 出力\n",
        "    worksheet.update_cell(i, 2, h1_title)  # B列にタイトルを書き込む\n",
        "    worksheet.update_cell(i, 3, headers_text)  # C列にH2H3情報を書き込む\n",
        "    worksheet.update_cell(i, 4, site_text_length)  # D列にサイトの文字数を書き込む\n",
        "    worksheet.update_cell(i, 5, description_content)  # E列にサイトのディスクリプションを書き込む\n",
        "    worksheet.update_cell(i, 6, f\"{external_links_count}\\n{external_links_text}\")  # F列に外部リンク数とリンクを書き込む\n",
        "\n",
        "    truncated_internal_links_text = internal_links_text[:50000]\n",
        "    worksheet.update_cell(i, 7, f\"{internal_links_count}\\n{truncated_internal_links_text}\")\n",
        "\n",
        "    # worksheet.update_cell(i, 7, f\"{internal_links_count}\\n{internal_links_text}\")  # G列に内部リンク数とリンクを書き込む\n",
        "\n",
        "    worksheet.update_cell(i, 8, article_links_count)  # H列にページ内リンク数を書き込む\n",
        "    worksheet.update_cell(i, 9, f\"{image_count}\\n{image_links_formatted}\")  # I列に画像数とリンクを書き込む\n",
        "    worksheet.update_cell(i, 10, keyword_counts_text)  # J列にキーワード出現数を書き込む\n",
        "\n",
        "    # APIの利用制限を考慮して待ち時間を設ける\n",
        "    time.sleep(2)\n"
      ],
      "metadata": {
        "id": "cM5HUOMnhmfl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8f2b8e1-baa7-43c3-98c4-ef4bc40fbe21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URLの取得が完了しました。\n",
            "Request error for https://exidea.co.jp/blog/job/job-change/it-tenshoku-agent/: HTTPSConnectionPool(host='exidea.co.jp', port=443): Max retries exceeded with url: /blog/job/job-change/it-tenshoku-agent/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\n"
          ]
        }
      ]
    }
  ]
}