{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ozawa-nami/test/blob/main/%E3%80%90%E5%85%B1%E6%9C%89%E7%94%A8%E3%80%91%E7%AB%B6%E5%90%88%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import gspread\n",
        "\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "\n",
        "\n",
        "!pip install selenium\n",
        "!pip install webdriver_manager pyvirtualdisplay\n",
        "!apt-get update\n",
        "!apt-get install -y xvfb wget\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!apt-get install -y ./google-chrome-stable_current_amd64.deb\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "# 仮想ディスプレイの起動\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "\n",
        "# Chrome のオプションとサービスの設定\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument('--headless')\n",
        "\n",
        "s = Service(ChromeDriverManager().install())\n",
        "\n",
        "# Chrome ブラウザの起動\n",
        "driver = webdriver.Chrome(service=s, options=options)\n"
      ],
      "metadata": {
        "id": "-ULOloWJhlQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#自分でURL入れる\n",
        "\n",
        "# 必要なライブラリのインストール\n",
        "!pip install gspread beautifulsoup4 requests\n",
        "\n",
        "import gspread\n",
        "import google.auth\n",
        "from google.colab import auth\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "import time\n",
        "import re\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import chardet\n",
        "import json  # JSONデータの処理用\n",
        "\n",
        "# Googleドライブの認証\n",
        "auth.authenticate_user()\n",
        "creds, _ = google.auth.default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "#★ スプレッドシートのURLとシート名\n",
        "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1OgdSqsKCGpteswYS2slz0fvwNi5kQzw8QDA1OWEWQQQ/edit#gid=1221780822\"\n",
        "worksheet_name = \"競合分析\"\n",
        "\n",
        "# スプレッドシートの読み込み\n",
        "sh = gc.open_by_url(spreadsheet_url)\n",
        "worksheet = sh.worksheet(worksheet_name)\n",
        "\n",
        "# A列からURLを取得（例として5行目から20行目までのURLを取得）\n",
        "urls = [worksheet.cell(i, 1).value for i in range(5, 25)]\n",
        "\n",
        "\n",
        "\n",
        "# classify_link関数の定義\n",
        "def classify_link(href, base_url):\n",
        "    parsed_url = urlparse(href)\n",
        "    if parsed_url.netloc == base_url:\n",
        "        return \"内部リンク\"\n",
        "    elif parsed_url.netloc:\n",
        "        return \"外部リンク\"\n",
        "    elif parsed_url.fragment:\n",
        "        return \"ページ内リンク\"\n",
        "    else:\n",
        "        return \"不明なリンク\"\n",
        "\n",
        "# A1セルからキーワードを取得\n",
        "keywords_string = worksheet.cell(1, 1).value\n",
        "# キーワードをカンマで区切ってリストに変換\n",
        "keywords_list = [keyword.strip() for keyword in keywords_string.split(',')]\n",
        "\n",
        "\n",
        "# 各URLに対してスクレイピングと情報取得\n",
        "for i, url in enumerate(urls, start=5):\n",
        "    if not url:\n",
        "        for j in range(2, 12):  # B列からL列まで\n",
        "            worksheet.update_cell(i, j, \"\")  # 空文字を書き込む\n",
        "        continue\n",
        "\n",
        "    # リクエストの送信\n",
        "    try:\n",
        "        res = requests.get(url)\n",
        "        res.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request error for {url}: {e}\")\n",
        "        for j in range(2, 12):  # B列からL列まで\n",
        "            worksheet.update_cell(i, j, \"\")  # 空文字を書き込む\n",
        "        continue\n",
        "\n",
        "    # ページの解析\n",
        "    soup = BeautifulSoup(res.text, 'html.parser')  # この行をforループ内に移動\n",
        "\n",
        "    # 文字コードを自動判別\n",
        "    detected_encoding = chardet.detect(res.content)['encoding']\n",
        "    # ページの解析\n",
        "    soup = BeautifulSoup(res.content, 'html.parser', from_encoding=detected_encoding)\n",
        "\n",
        "    # サイトのテキストを取得\n",
        "    site_text = soup.get_text()\n",
        "\n",
        "    # H1タイトル\n",
        "    h1_tag = soup.find('h1')\n",
        "    h1_title = h1_tag.text.strip() if h1_tag else \"N/A\"\n",
        "\n",
        "    # H2とH3の見出しの取得\n",
        "    headers = soup.find_all(['h2', 'h3'])\n",
        "    headers_list = []\n",
        "    for h in headers:\n",
        "        if h.name == 'h2':\n",
        "            headers_list.append(f\"H2: {h.text.strip()}\")\n",
        "        elif h.name == 'h3':\n",
        "            headers_list.append(f\"H3: {h.text.strip()}\")\n",
        "    headers_text = \"\\n\".join(headers_list)\n",
        "\n",
        "    # サイトの文字数\n",
        "    site_text_length = len(re.sub(r'\\s+', ' ', site_text))\n",
        "\n",
        "    # ディスクリプション（metaタグから取得）\n",
        "    description = soup.find('meta', attrs={'name': 'description'})\n",
        "    description_content = description.get('content').strip() if description else \"N/A\"\n",
        "\n",
        "    # リンク数のカウント\n",
        "    links = soup.find_all('a')\n",
        "    link_count = len(links)\n",
        "\n",
        "\n",
        "\n",
        "  # classify_link関数の定義\n",
        "\n",
        "    def classify_link(href, base_url):\n",
        "        parsed_url = urlparse(href)\n",
        "        if parsed_url.netloc == base_url:\n",
        "            return \"内部リンク\"\n",
        "        elif parsed_url.netloc and base_url not in parsed_url.netloc:\n",
        "            return \"外部リンク\"\n",
        "        elif parsed_url.path.startswith(\"/\"):\n",
        "            return \"内部リンク\"\n",
        "        elif parsed_url.fragment:\n",
        "            return \"ページ内リンク\"\n",
        "        else:\n",
        "            return \"不明なリンク\"\n",
        "\n",
        "\n",
        "    # 発リンクおよび内部リンクのカウント\n",
        "    base_url = urlparse(url).netloc\n",
        "    internal_links = []\n",
        "    external_links = []\n",
        "    article_links = []\n",
        "\n",
        "    for link in links:\n",
        "        href = link.get('href')\n",
        "        if href:\n",
        "            link_type = classify_link(href, base_url)\n",
        "            if link_type == \"内部リンク\":\n",
        "                internal_links.append((href, link.text))\n",
        "            elif link_type == \"外部リンク\":\n",
        "                external_links.append((href, link.text))\n",
        "            elif link_type == \"ページ内リンク\":\n",
        "                article_links.append((href, link.text))\n",
        "\n",
        "    # 外部リンクの整形\n",
        "    external_links_count = len(external_links)\n",
        "    external_links_text = \"\\n\".join([f\"{text}: {href}\" for href, text in external_links])\n",
        "\n",
        "    # 内部リンクの整形\n",
        "    internal_links_count = len(internal_links)\n",
        "    internal_links_text = \"\\n\".join([f\"{text}: {href}\" for href, text in internal_links])\n",
        "\n",
        "    # 記事内リンクの整形\n",
        "    article_links_count = len(article_links)\n",
        "    article_links_text = \"\\n\".join([f\"{text}: {href}\" for href, text in article_links])\n",
        "\n",
        "\n",
        "    # 画像リンクとその数の取得\n",
        "    images = soup.find_all('img')\n",
        "    image_count = len(images)\n",
        "    image_links = [img.get('src') for img in images if img.get('src')]\n",
        "    image_links_formatted = \"\\n\".join(image_links)\n",
        "\n",
        "    # 画像リンクのテキストを切り詰める\n",
        "    truncated_image_links_formatted = image_links_formatted[:50000]\n",
        "\n",
        "\n",
        "    # JSONデータを抽出\n",
        "    soup = BeautifulSoup(res.content, 'html.parser')\n",
        "    json_ld_scripts = soup.find_all('script', {'type': 'application/ld+json'})\n",
        "\n",
        "    # JSONデータが複数ある場合の処理\n",
        "    date_published = \"N/A\"\n",
        "    date_modified = \"N/A\"\n",
        "\n",
        "    for script in json_ld_scripts:\n",
        "        try:\n",
        "            json_data = json.loads(script.string)\n",
        "            if isinstance(json_data, list):\n",
        "                # JSONデータがリストの場合、各要素をチェック\n",
        "                for item in json_data:\n",
        "                    if item.get('@type') in ['BlogPosting', 'Article', 'WebPage']:\n",
        "                        date_published = item.get('datePublished', \"N/A\")\n",
        "                        date_modified = item.get('dateModified', \"N/A\")\n",
        "                        break\n",
        "            elif json_data.get('@type') in ['BlogPosting', 'Article', 'WebPage']:\n",
        "                date_published = json_data.get('datePublished', \"N/A\")\n",
        "                date_modified = json_data.get('dateModified', \"N/A\")\n",
        "            elif json_data.get('@graph'):\n",
        "                # グラフ構造を持つJSONデータの場合\n",
        "                graph_data = json_data.get('@graph')\n",
        "                for item in graph_data:\n",
        "                    if item.get('@type') in ['BlogPosting', 'Article', 'WebPage']:\n",
        "                        date_published = item.get('datePublished', \"N/A\")\n",
        "                        date_modified = item.get('dateModified', \"N/A\")\n",
        "                        break\n",
        "        except json.JSONDecodeError:\n",
        "            # JSONデータのパースに失敗した場合\n",
        "            continue\n",
        "        if date_published != \"N/A\" and date_modified != \"N/A\":\n",
        "            # 有効なデータが見つかった場合\n",
        "            break\n",
        "\n",
        "    # date_publishedとdate_modifiedを出力\n",
        "    print(f\"URL: {url}\")\n",
        "    print(f\"Date Published: {date_published}\")\n",
        "    print(f\"Date Modified: {date_modified}\")\n",
        "\n",
        "\n",
        "    # 各キーワードの出現数をカウント\n",
        "    keyword_counts = {}\n",
        "    for keyword in keywords_list:\n",
        "        keyword_count = site_text.lower().count(keyword.lower())  # 大小文字を無視してカウント\n",
        "        keyword_counts[keyword] = keyword_count\n",
        "    keyword_counts_text = \"\\n\".join([f\"{k}: {v}\" for k, v in keyword_counts.items()])\n",
        "\n",
        "    # 出力\n",
        "    worksheet.update_cell(i, 2, h1_title)  # B列にタイトルを書き込む\n",
        "    worksheet.update_cell(i, 3, headers_text)  # C列にH2H3情報を書き込む\n",
        "    worksheet.update_cell(i, 4, site_text_length)  # D列にサイトの文字数を書き込む\n",
        "    worksheet.update_cell(i, 5, description_content)  # E列にサイトのディスクリプションを書き込む\n",
        "    worksheet.update_cell(i, 6, f\"{external_links_count}\\n{external_links_text}\")  # F列に外部リンク数とリンクを書き込む\n",
        "\n",
        "    truncated_internal_links_text = internal_links_text[:50000]\n",
        "    worksheet.update_cell(i, 7, f\"{internal_links_count}\\n{truncated_internal_links_text}\")   # G列に内部リンク数とリンクを書き込む\n",
        "\n",
        "    # worksheet.update_cell(i, 7, f\"{internal_links_count}\\n{internal_links_text}\")  # G列に内部リンク数とリンクを書き込む\n",
        "\n",
        "    worksheet.update_cell(i, 8, article_links_count)  # H列にページ内リンク数を書き込む\n",
        "\n",
        "        # I列に画像数とリンクを書き込む（画像リンクのテキストを切り詰めたバージョンを使用）\n",
        "    worksheet.update_cell(i, 9, f\"{image_count}\\n{truncated_image_links_formatted}\")\n",
        "\n",
        "    worksheet.update_cell(i, 10, date_published)  # J列にDate Publishedを書き込む\n",
        "    worksheet.update_cell(i, 11, date_modified)  # K列にDate Modifiedを書き込む\n",
        "    worksheet.update_cell(i, 12, keyword_counts_text)  # L列にキーワード出現数を書き込む\n",
        "\n",
        "\n",
        "    # APIの利用制限を考慮して待ち時間を設ける\n",
        "    time.sleep(2)\n"
      ],
      "metadata": {
        "id": "DB9bzgxeq0Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A1セルに入っているKWの上位⚪︎位を分析\n",
        "\n",
        "import gspread\n",
        "import google.auth\n",
        "from google.colab import auth\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "import time\n",
        "import re\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import chardet\n",
        "\n",
        "# Googleドライブの認証\n",
        "auth.authenticate_user()\n",
        "creds, _ = google.auth.default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "#★ スプレッドシートのURLとシート名\n",
        "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1OgdSqsKCGpteswYS2slz0fvwNi5kQzw8QDA1OWEWQQQ/edit#gid=1221780822\"\n",
        "worksheet_name = \"競合分析\"\n",
        "\n",
        "# スプレッドシートの読み込み\n",
        "sh = gc.open_by_url(spreadsheet_url)\n",
        "worksheet = sh.worksheet(worksheet_name)\n",
        "\n",
        "# キーワードの取得\n",
        "keyword = worksheet.acell('A1').value\n",
        "keywords_list = keyword.split(\",\")\n",
        "\n",
        "# Google検索のURLを構築\n",
        "search_url = f\"https://www.google.com/search?q={keyword}&num=10\"\n",
        "\n",
        "# リクエストの送信\n",
        "try:\n",
        "    res = requests.get(search_url)\n",
        "    res.raise_for_status()\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Request error: {e}\")\n",
        "    exit()\n",
        "\n",
        "# ページの解析\n",
        "soup = BeautifulSoup(res.text, 'html.parser')\n",
        "\n",
        "# URLの取得\n",
        "url_cells = soup.select('.kCrYT > a')\n",
        "urls = []\n",
        "for url_cell in url_cells:\n",
        "    href = url_cell.get('href')\n",
        "    parsed_url = urlparse(href)\n",
        "    query_params = parse_qs(parsed_url.query)\n",
        "    if 'q' in query_params:\n",
        "        url = query_params['q'][0]\n",
        "        urls.append(url)\n",
        "\n",
        "# classify_link関数の定義\n",
        "def classify_link(href, base_url):\n",
        "    parsed_url = urlparse(href)\n",
        "    if parsed_url.netloc == base_url:\n",
        "        return \"内部リンク\"\n",
        "    elif parsed_url.netloc:\n",
        "        return \"外部リンク\"\n",
        "    elif parsed_url.fragment:\n",
        "        return \"ページ内リンク\"\n",
        "    else:\n",
        "        return \"不明なリンク\"\n",
        "\n",
        "# 上位10記事のURLをスプレッドシートに書き込む\n",
        "for i, url in enumerate(urls, start=5):\n",
        "    worksheet.update_cell(i, 1, url)\n",
        "\n",
        "print(\"URLの取得が完了しました。\")\n",
        "\n",
        "# 各URLに対してスクレイピングと情報取得\n",
        "for i, url in enumerate(urls, start=5):\n",
        "    if not url:\n",
        "        for j in range(2, 11):  # B列からJ列まで\n",
        "            worksheet.update_cell(i, j, \"\")  # 空文字を書き込む\n",
        "        continue\n",
        "\n",
        "    # リクエストの送信\n",
        "    try:\n",
        "        res = requests.get(url)\n",
        "        res.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request error for {url}: {e}\")\n",
        "        for j in range(2, 11):  # B列からJ列まで\n",
        "            worksheet.update_cell(i, j, \"\")  # 空文字を書き込む\n",
        "        continue\n",
        "\n",
        "    # 文字コードを自動判別\n",
        "    detected_encoding = chardet.detect(res.content)['encoding']\n",
        "    # ページの解析\n",
        "    soup = BeautifulSoup(res.content, 'html.parser', from_encoding=detected_encoding)\n",
        "\n",
        "    # サイトのテキストを取得\n",
        "    site_text = soup.get_text()\n",
        "\n",
        "    # H1タイトル\n",
        "    h1_tag = soup.find('h1')\n",
        "    h1_title = h1_tag.text.strip() if h1_tag else \"N/A\"\n",
        "\n",
        "    # H2とH3の見出しの取得\n",
        "    headers = soup.find_all(['h2', 'h3'])\n",
        "    headers_list = []\n",
        "    for h in headers:\n",
        "        if h.name == 'h2':\n",
        "            headers_list.append(f\"H2: {h.text.strip()}\")\n",
        "        elif h.name == 'h3':\n",
        "            headers_list.append(f\"H3: {h.text.strip()}\")\n",
        "    headers_text = \"\\n\".join(headers_list)\n",
        "\n",
        "    # サイトの文字数\n",
        "    site_text_length = len(re.sub(r'\\s+', ' ', site_text))\n",
        "\n",
        "    # ディスクリプション（metaタグから取得）\n",
        "    description = soup.find('meta', attrs={'name': 'description'})\n",
        "    description_content = description.get('content').strip() if description else \"N/A\"\n",
        "\n",
        "    # リンク数のカウント\n",
        "    links = soup.find_all('a')\n",
        "    link_count = len(links)\n",
        "\n",
        "\n",
        "\n",
        "  # classify_link関数の定義\n",
        "    def classify_link(href, base_url):\n",
        "        parsed_url = urlparse(href)\n",
        "        if parsed_url.netloc == base_url:\n",
        "            return \"内部リンク\"\n",
        "        elif parsed_url.netloc and base_url not in parsed_url.netloc:\n",
        "            return \"外部リンク\"\n",
        "        elif parsed_url.path.startswith(\"/\"):\n",
        "            return \"内部リンク\"\n",
        "        elif parsed_url.fragment:\n",
        "            return \"ページ内リンク\"\n",
        "        else:\n",
        "            return \"不明なリンク\"\n",
        "\n",
        "\n",
        "    # 発リンクおよび内部リンクのカウント\n",
        "    base_url = urlparse(url).netloc\n",
        "    internal_links = []\n",
        "    external_links = []\n",
        "    article_links = []\n",
        "\n",
        "    for link in links:\n",
        "        href = link.get('href')\n",
        "        if href:\n",
        "            link_type = classify_link(href, base_url)\n",
        "            if link_type == \"内部リンク\":\n",
        "                internal_links.append((href, link.text))\n",
        "            elif link_type == \"外部リンク\":\n",
        "                external_links.append((href, link.text))\n",
        "            elif link_type == \"ページ内リンク\":\n",
        "                article_links.append((href, link.text))\n",
        "\n",
        "    # 外部リンクの整形\n",
        "    external_links_count = len(external_links)\n",
        "    external_links_text = \"\\n\".join([f\"{text}: {href}\" for href, text in external_links])\n",
        "\n",
        "    # 内部リンクの整形\n",
        "    internal_links_count = len(internal_links)\n",
        "    internal_links_text = \"\\n\".join([f\"{text}: {href}\" for href, text in internal_links])\n",
        "\n",
        "    # 記事内リンクの整形\n",
        "    article_links_count = len(article_links)\n",
        "    article_links_text = \"\\n\".join([f\"{text}: {href}\" for href, text in article_links])\n",
        "\n",
        "    # 画像リンクとその数の取得\n",
        "    images = soup.find_all('img')\n",
        "    image_count = len(images)\n",
        "    image_links = [img.get('src') for img in images if img.get('src')]\n",
        "    image_links_formatted = \"\\n\".join(image_links)\n",
        "\n",
        "    # 各キーワードの出現数をカウント\n",
        "    keyword_counts = {}\n",
        "    for keyword in keywords_list:\n",
        "        keyword_count = site_text.count(keyword)\n",
        "        keyword_counts[keyword] = keyword_count\n",
        "    keyword_counts_text = \"\\n\".join([f\"{k}: {v}\" for k, v in keyword_counts.items()])\n",
        "\n",
        "    # 出力\n",
        "    worksheet.update_cell(i, 2, h1_title)  # B列にタイトルを書き込む\n",
        "    worksheet.update_cell(i, 3, headers_text)  # C列にH2H3情報を書き込む\n",
        "    worksheet.update_cell(i, 4, site_text_length)  # D列にサイトの文字数を書き込む\n",
        "    worksheet.update_cell(i, 5, description_content)  # E列にサイトのディスクリプションを書き込む\n",
        "    worksheet.update_cell(i, 6, f\"{external_links_count}\\n{external_links_text}\")  # F列に外部リンク数とリンクを書き込む\n",
        "\n",
        "    truncated_internal_links_text = internal_links_text[:50000]\n",
        "    worksheet.update_cell(i, 7, f\"{internal_links_count}\\n{truncated_internal_links_text}\")\n",
        "\n",
        "    # worksheet.update_cell(i, 7, f\"{internal_links_count}\\n{internal_links_text}\")  # G列に内部リンク数とリンクを書き込む\n",
        "\n",
        "    worksheet.update_cell(i, 8, article_links_count)  # H列にページ内リンク数を書き込む\n",
        "    worksheet.update_cell(i, 9, f\"{image_count}\\n{image_links_formatted}\")  # I列に画像数とリンクを書き込む\n",
        "    worksheet.update_cell(i, 10, keyword_counts_text)  # J列にキーワード出現数を書き込む\n",
        "\n",
        "    # APIの利用制限を考慮して待ち時間を設ける\n",
        "    time.sleep(2)\n"
      ],
      "metadata": {
        "id": "cM5HUOMnhmfl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}